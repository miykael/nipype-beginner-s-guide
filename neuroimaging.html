<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">



<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Nipype Beginner&#39;s Guide &mdash; All you need to know to become an expert in Nipype</title>
    
    <link rel="stylesheet" href="_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/graphviz.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="search" title="Search" href="search.html" />
    <link rel="top" title="All you need to know to become an expert in Nipype" href="index.html" />
    <link rel="next" title="Nipype and Neuroimaging" href="nipypeAndNeuroimaging.html" />
    <link rel="prev" title="Introduction to Nipype" href="nipype.html" />
    <meta name="keywords" content="nipype, neuroimaging, pipeline, workflow, parallel, python, neuroscience, python, guide, mri, fmri, dti, tutorial, user guide">
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-24958678-1', 'auto');
      ga('send', 'pageview');

    </script>

  </head>
  <body>

    <div class="headertext" >
     <a href="index.html">
        Nipype Beginner's Guide</a>
    </div>

    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="nipypeAndNeuroimaging.html" title="Nipype and Neuroimaging"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="nipype.html" title="Introduction to Nipype"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">Home</a>|</li>
        <li><a href="http://miykael.github.com/nipype-beginner-s-guide/tableofcontent.html">Table of Contents</a>|</li>
        <li><a href="http://miykael.github.com/nipype-beginner-s-guide/faq.html">FAQ</a>|</li>
        <li><a href="http://miykael.github.com/nipype-beginner-s-guide/glossary.html">Glossary</a>|</li>
        <li><a href="https://github.com/miykael/nipype-beginner-s-guide/">github</a>|</li>
        <li><a href="http://nipy.org/nipype/">Nipype</a>|</li>
        <li><a href="http://miykael.github.io/nipype-beginner-s-guide/help.html">Help</a>|</li> 
      </ul>
    </div>
  
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/nipype-beginners-guide-html_logo.png" alt="Logo"/>
            </a></p>
  <h3><a href="index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Introduction to Neuroimaging</a><ul>
<li><a class="reference internal" href="#acquisition-of-mri-data">Acquisition of MRI Data</a></li>
<li><a class="reference internal" href="#specifics-of-mri-data">Specifics of MRI Data</a></li>
<li><a class="reference internal" href="#modalities-of-mri-data">Modalities of MRI Data</a><ul>
<li><a class="reference internal" href="#smri-structural-mri">sMRI (structural MRI)</a></li>
<li><a class="reference internal" href="#fmri-functional-mri">fMRI (functional MRI)</a></li>
<li><a class="reference internal" href="#dmri-diffusion-mri">dMRI (diffusion MRI)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#analysis-steps">Analysis Steps</a><ul>
<li><a class="reference internal" href="#step-1-preprocessing">Step 1: Preprocessing</a><ul>
<li><a class="reference internal" href="#slice-timing-correction-fmri-only">Slice Timing Correction (fMRI only)</a></li>
<li><a class="reference internal" href="#motion-correction-fmri-only">Motion Correction (fMRI only)</a></li>
<li><a class="reference internal" href="#artifact-detection-fmri-only">Artifact Detection (fMRI only)</a></li>
<li><a class="reference internal" href="#coregistration">Coregistration</a></li>
<li><a class="reference internal" href="#normalization">Normalization</a></li>
<li><a class="reference internal" href="#smoothing">Smoothing</a></li>
<li><a class="reference internal" href="#segmentation-smri-only">Segmentation (sMRI only)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#step-2-model-specification-and-estimation">Step 2: Model Specification and Estimation</a><ul>
<li><a class="reference internal" href="#the-general-linear-model">The General Linear Model</a></li>
<li><a class="reference internal" href="#potential-problems-of-the-glm-approach">Potential problems of the GLM approach</a></li>
<li><a class="reference internal" href="#example-of-a-design-matrix">Example of a Design Matrix</a></li>
<li><a class="reference internal" href="#model-estimation">Model Estimation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#step-3-statistical-inference">Step 3: Statistical Inference</a><ul>
<li><a class="reference internal" href="#contrast-estimation">Contrast Estimation</a></li>
<li><a class="reference internal" href="#thresholding">Thresholding</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="nipype.html"
                        title="previous chapter">Introduction to Nipype</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="nipypeAndNeuroimaging.html"
                        title="next chapter">Nipype and Neuroimaging</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/neuroimaging.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
        </div>
      </div>
    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">

            
  <div class="admonition important">
<p class="admonition-title">Important</p>
<p>This guide hasn’t been updated since January 2017 and is based on an older version of Nipype. The code in this guide is not tested against newer Nipype versions and might not work anymore. For a newer, more up to date and better introduction to Nipype, please check out the the <a class="reference external" href="https://miykael.github.io/nipype_tutorial/">Nipype Tutorial</a>.</p>
</div>
<div class="section" id="introduction-to-neuroimaging">
<h1>Introduction to Neuroimaging<a class="headerlink" href="#introduction-to-neuroimaging" title="Permalink to this headline">¶</a></h1>
<p>In this section, I will introduce you to the basics of analyzing neuroimaging data. I will give a brief explanation of how the neuroimaging data is acquired, how the data is prepared for analysis (also called preprocessing). Finally, I’ll show how you can analyze your data using a model based on your hypothesis.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This part is only a brief introduction to neuroimaging. Further information on this topic can be found under the <a class="reference external" href="http://miykael.github.io/nipype-beginner-s-guide/glossary.html">Glossary</a> and <a class="reference external" href="http://miykael.github.io/nipype-beginner-s-guide/faq.html">FAQ</a> pages of this beginner’s guide.</p>
</div>
<div class="section" id="acquisition-of-mri-data">
<h2>Acquisition of MRI Data<a class="headerlink" href="#acquisition-of-mri-data" title="Permalink to this headline">¶</a></h2>
<a class="reference internal image-reference" href="_images/brain.gif"><img alt="_images/brain.gif" class="align-left" src="_images/brain.gif" style="width: 160pt;" /></a>
<p>The technology and physics behind an MRI scanner is quite astonishing. But I won’t go into the details of how it all works. You do need to know some terms, concepts, and parameters that are used to acquire MRI data in a scanning session and use that to construct useable images.</p>
<p>The brain occupies space, so when we collect data on how it fills space, we call that volume data, and all the volume data needed to create the complete, 3D image of the brain, recorded at one single timepoint and as pictured on the left, is called a <strong>volume</strong>. The data is measured in <strong>voxels</strong>, which are like the pixels used to display images on your screen, only in 3D. Each voxel has a specific dimension, in this case it is 1mm x 1mm x 1mm: a cube, so it is the same dimension from all sides (isotropic). Each voxel contains one value which stands for the average signal measured at the given location.</p>
<p>A standard anatomical volume, with an isotropic voxel resolution of 1mm contains almost 17 million voxels, which are arranged in a <strong>3D matrix</strong> of 256 x 256 x 256 voxels. The following picture shows a slice – one layer of the big, 3D matrix – through a brain volume, and the superimposed grid shows the remaining two dimensions of the voxels.</p>
<a class="reference internal image-reference" href="_images/voxel.png"><img alt="_images/voxel.png" class="align-center" src="_images/voxel.png" style="width: 450pt;" /></a>
<p>As the scanner can’t measure the whole volume at once it has to measure portions of the brain sequentially in time. This is done by measuring one plane of the brain (generally the horizontal one) after the other. Such a plane is also called a <strong>slice</strong>. The <strong>resolution</strong> of the measured volume data, therefore, depends on the in-plane resolution (the size of the squares in the above image), the number of slices and their thickness (how many layers), and any possible gaps between the layers.</p>
<p>The quality of the measured data depends on the resolution and the following parameters:</p>
<ul class="simple">
<li><p><strong>repetition time (TR)</strong>: time required to scan one volume</p></li>
<li><p><strong>acquisition time (TA)</strong>: time required to scan one slice. TA = TR - (TR/number of slices)</p></li>
<li><p><strong>field of view (FOV)</strong>: defines the extent of a slice, e.g. 256mm x 256mm</p></li>
</ul>
</div>
<div class="section" id="specifics-of-mri-data">
<h2>Specifics of MRI Data<a class="headerlink" href="#specifics-of-mri-data" title="Permalink to this headline">¶</a></h2>
<p>MRI scanners output their neuroimaging data in a raw data format with which most analysis packages cannot work. <strong>DICOM</strong> is a common, standardized, raw medical image format, but the format of your raw data may be something else; e.g., <strong>PAR/REC</strong> format from Philips scanners. Raw data is saved in <a class="reference external" href="https://en.wikipedia.org/wiki/K-space_%28magnetic_resonance_imaging%29">k-space</a> format, and it needs to be converted into a format that the analysis packages can use. The most frequent format for newly generated data is called <a class="reference external" href="http://nifti.nimh.nih.gov/">NIfTI</a>. If you are working with older datasets, you may encounter data in <strong>Analyze</strong> format. MRI data formats will have an <strong>image</strong> and a <strong>header</strong> part. For NifTI format, they are in the same file (.nii-file), whereas in the older Analyze format, they are in separate files (.img and .hdr-file).</p>
<ul class="simple">
<li><p>The <strong>image</strong> is the actual data and is represented by a 3D matrix that contains a value (e.g. gray value) for each voxel.</p></li>
<li><p>The <strong>header</strong> contains information about the data like voxel dimension, voxel extend in each dimension, number of measured time points, a transformation matrix that places the 3D matrix from the <strong>image</strong> part in a 3D coordinate system, etc.</p></li>
</ul>
</div>
<div class="section" id="modalities-of-mri-data">
<h2>Modalities of MRI Data<a class="headerlink" href="#modalities-of-mri-data" title="Permalink to this headline">¶</a></h2>
<p>There are many different kinds of acquisition techniques. But the most common ones are structural magnetic resonance imaging (<strong>sMRI</strong>), functional magnetic resonance imaging (<strong>fMRI</strong>) and diffusion tensor imaging (<strong>DTI</strong>).</p>
<div class="section" id="smri-structural-mri">
<h3>sMRI (structural MRI)<a class="headerlink" href="#smri-structural-mri" title="Permalink to this headline">¶</a></h3>
<a class="reference internal image-reference" href="_images/GM.gif"><img alt="_images/GM.gif" class="align-left" src="_images/GM.gif" style="width: 270pt;" /></a>
<p>Structural magnetic resonance imaging (<strong>sMRI</strong>) is a technique for measuring the anatomy of the brain. By measuring the amount of water at a given location, sMRI is capable of acquiring a detailed anatomical picture of our brain. This allows us to accurately distinguish between different types of tissue, such as gray and white matter. Structural images are high-resolution images of the brain that are used as reference images for multiple purposes, such as corregistration, normalization, segmentation, and surface reconstruction.</p>
<p>As there is no time pressure during acquisition of anatomical images (the anatomy is not supposed to change while the person is in the scanner), a higher resolution can be used for recording anatomical images, with a voxel extent of 0.2 to 1.5mm, depending on the strength of the magnetic field in the scanner, e.g. 1.5T, 3T or 7T. Grey matter structures are seen in dark, and the white matter structures in bright colors.</p>
</div>
<div class="section" id="fmri-functional-mri">
<h3>fMRI (functional MRI)<a class="headerlink" href="#fmri-functional-mri" title="Permalink to this headline">¶</a></h3>
<a class="reference internal image-reference" href="_images/BOLDresponse.png"><img alt="_images/BOLDresponse.png" class="align-right" src="_images/BOLDresponse.png" style="width: 270pt;" /></a>
<p>Functional magnetic resonance imaging (<strong>fMRI</strong>) is a technique for measuring brain activity. It works by detecting the changes in blood oxygenation and blood flow that occur in response to neural activity. Our brain is capable of so many astonishing things. But as nothing comes from nothing, it needs a lot of energy to sustain its functionality, and increased activity at a location increases the local energy consumption in the form of oxygen (O2) which is carried by the blood. Therefore, increased function results in increased blood flow towards the energy consuming location.</p>
<p>Immediately after neural activity the blood oxygen level decreases, known as the <em>initial dip</em>, because of the local energy consumption. This is followed by increased flow of new and oxygen-rich blood towards the energy consuming region. After 4-6 seconds a peak of blood oxygen level is reached. After no further neuronal activation takes place the signal decreases again and typically undershoots, before rising again to the baseline level.</p>
<p>The blood oxygen level is exactly what we measure with fMRI. The MRI Scanner is able to measure the changes in the magnetic field caused by the difference in the  magnetic susceptibility of oxygenated (diamagnetic) and deoxygenated (paramagnetic) blood. The signal is therefore called the <strong>Blood Oxygen Level Dependent (BOLD) response</strong>.</p>
<a class="reference internal image-reference" href="_images/WM.gif"><img alt="_images/WM.gif" class="align-left" src="_images/WM.gif" style="width: 270pt;" /></a>
<p>Because the BOLD signal has to be measured quickly, the resolution of functional images is normally lower (2-4mm) than the resolution of structural images (0.5-1.5mm). But this depends strongly on the strength of the magnetic field in the scanner, e.g. 1.5T, 3T or 7T. In a functional image, the gray matter is seen as bright and the white matter as dark colors, which is the exact opposite to structural images.</p>
<p>Depending on the paradigm, we talk about <strong>event-related</strong>, <strong>block</strong> or <strong>resting-state</strong> designs.</p>
<ul class="simple">
<li><p><strong>event-related design</strong>: Event-related means that stimuli are administered to the subjects in the scanner for a short period. The stimuli are only administered briefly and generally in random order. Stimuli are typically visual, but audible or or other sensible stimuli could also be used. This means that the BOLD response consists of short bursts of activity, which should manifest as peaks, and should look more or less like the line shown in the graph above.</p></li>
<li><p><strong>block design</strong>: If multiple stimuli of a similar nature are shown in a block, or phase, of 10-30 seconds, that is a block design. Such a design has the advantages that the peak in the BOLD signal is not just attained for a short period but elevated for a longer time, creating a plateau in the graph. This makes it easier to detect an underlying activation increase.</p></li>
<li><p><strong>resting-state design</strong>: Resting-state designs acquire data in the absence of stimulation. Subjects are asked to lay still and rest in the scanner without falling asleep. The goal of such a scan is to record brain activation in the absence of an external task. This is sometimes done to analyze the functional connectivity of the brain.</p></li>
</ul>
</div>
<div class="section" id="dmri-diffusion-mri">
<h3>dMRI (diffusion MRI)<a class="headerlink" href="#dmri-diffusion-mri" title="Permalink to this headline">¶</a></h3>
<img alt="_images/tractography_small.gif" class="align-left" src="_images/tractography_small.gif" />
<p>Diffusion imaging is done to obtain information about the brain’s white matter connections. There are multiple modalities to record diffusion images, such as diffusion tensor imaging (DTI), diffusion spectrum imaging (DSI), diffusion weighted imaging (DWI) and diffusion functional MRI (DfMRI). By recording the diffusion trajectory of the molecules (usually water) in a given voxel, one can make inferences about the underlying structure in the voxel. For example, if one voxel contains mostly horizontal fiber tracts, the water molecules in this region will mostly diffuse (move) in a horizontal manner, as they can’t move vertically because of this neural barrier. The diffusion itself is mostly a <a class="reference external" href="https://en.wikipedia.org/wiki/Brownian_motion">Brownian motion</a>.</p>
<p>There are many different <a class="reference external" href="https://en.wikipedia.org/wiki/Diffusion_MRI#Measures_of_anisotropy_and_diffusivity">diffusion measurements</a>, such as <strong>mean diffusivity</strong> (MD), <a class="reference external" href="https://en.wikipedia.org/wiki/Fractional_anisotropy">fractional anisotropy</a> (FA) and <a class="reference external" href="https://en.wikipedia.org/wiki/Tractography">Tractography</a>. Each measurement gives different insights into the brain’s neural fiber tracts. An example of a reconstructed tractography can be seen in the image to the left.</p>
<p>Diffusion MRI is a rather new field in MRI and still has some problems with its sensitivity to correctly detect fiber tracts and their underlying orientation. For example, the standard DTI method has almost no chance of reliably detecting kissing (touching) or crossing fiber tracts. To account for this disadvantage, newer methods such as <strong>High-angular-resolution diffusion imaging</strong> (HARDI) and Q-ball vector analysis were developed. For more about diffusion MRI see the <a class="reference external" href="https://en.wikipedia.org/wiki/Diffusion_MRI">Diffusion MRI Wikipedia page</a>.</p>
</div>
</div>
<div class="section" id="analysis-steps">
<h2>Analysis Steps<a class="headerlink" href="#analysis-steps" title="Permalink to this headline">¶</a></h2>
<p>There are many different steps involved in a neuroimaging analysis and there is not just one order in which to perform them. Depending on the researcher, the paradigm at hand, or the modality analyzed (sMRI, fMRI, dMRI), the order can differ. Some steps may occur earlier or later or may be left out entirely. Nonetheless, the general procedure for fMRI analysis can be divided into the following three steps:</p>
<ol class="arabic simple">
<li><p><strong>Preprocessing</strong>: Spatial and temporal preprocessing of the data to prepare it for the 1st and 2nd level inferential analysis</p></li>
<li><p><strong>Model Specification and Estimation</strong>: Specifying and estimating parameters of the statistical model</p></li>
<li><p><strong>Statistical Inference</strong>: Making inferences about the estimated parameters using appropriate statistical methods</p></li>
</ol>
<div class="section" id="step-1-preprocessing">
<h3>Step 1: Preprocessing<a class="headerlink" href="#step-1-preprocessing" title="Permalink to this headline">¶</a></h3>
<p>Preprocessing is the term used to for all the steps taken to improve our data and prepare it for statistical analysis. We may correct or adjust our data for a number of things inherent in the experimental situation:  to take account of time differences between acquiring each image slice, to correct for head movement during scanning, to detect ‘artifacts’ – anomalous measurements – that should be excluded from subsequent analysis; to align the functional images with the reference structural image, and to normalize the data into a standard space so that data can be compared among several subjects; to apply filtering to the image to increase the signal-to-noise ratio; finally, if sMRI is intended, a segmentation step may be performed. We will now look at each of those steps in more detail.</p>
<div class="section" id="slice-timing-correction-fmri-only">
<h4>Slice Timing Correction (fMRI only)<a class="headerlink" href="#slice-timing-correction-fmri-only" title="Permalink to this headline">¶</a></h4>
<a class="reference internal image-reference" href="_images/slicetiming_small.gif"><img alt="_images/slicetiming_small.gif" class="align-right" src="_images/slicetiming_small.gif" style="width: 499px;" /></a>
<p>Because functional MRI measurement sequences don’t acquire every slice in a volume at the same time we have to account for the time differences among the slices. For example, if you acquire a volume with 37 slices in ascending order, and each slice is acquired every 50ms, there is a difference of 1.8s between the first and the last slice acquired. You must know the order in which the slices were acquired to be able to apply the proper correction. Slices are typically acquired in one of three methods:  descending order (top-down); ascending order (bottom-up); or interleaved (acquire every other slice in each direction), where the interleaving may start at the top or the bottom. (Left: <em>ascending</em>, Right: <em>interleaved</em>)</p>
<p>Slice Timing Correction is used to compensate for the time differences between the slice acquisitions by temporally interpolating the slices so that the resulting volume is close to equivalent to acquiring the whole brain image at a single time point. This temporal factor of acquisition especially has to be accounted for in fMRI models where timing is an important factor (e.g. for event related designs, where the type of stimulus changes from volume to volume).</p>
</div>
<div class="section" id="motion-correction-fmri-only">
<h4>Motion Correction (fMRI only)<a class="headerlink" href="#motion-correction-fmri-only" title="Permalink to this headline">¶</a></h4>
<a class="reference internal image-reference" href="_images/movement.gif"><img alt="_images/movement.gif" class="align-right" src="_images/movement.gif" style="width: 200pt;" /></a>
<p>Motion correction, also known as Realignment, is used to correct for head movement during the acquisition of functional data. Even small head movements lead to unwanted variation in voxels and reduce the quality of your data. Motion correction tries to minimize the influence of movement on your data by aligning your data to a reference time volume. This reference time volume is usually the mean image of all timepoints, but it could also be the first, or some other, time point.</p>
<p>Head movement can be characterized by six parameters:  Three translation parameters which code movement in the directions of the three dimensional axes, movement along the X, Y, or Z axes; and three rotation parameters which code rotation about those axes, rotation centered on each of the X, Y, and Z axes).</p>
<p>Realignment usually uses an affine rigid body transformation to manipulate the data in those six parameters. That is, each image can be moved but not distorted to best align with all the other images. Below you see a plot of a “good” subject where the movement is minimal.</p>
<a class="reference internal image-reference" href="_images/realignment_good.png"><img alt="_images/realignment_good.png" class="align-center" src="_images/realignment_good.png" style="width: 400pt;" /></a>
</div>
<div class="section" id="artifact-detection-fmri-only">
<h4>Artifact Detection (fMRI only)<a class="headerlink" href="#artifact-detection-fmri-only" title="Permalink to this headline">¶</a></h4>
<p>Almost no subjects lie perfectly still. As we can see from the sharp spikes in the graphs below, some move quite drastically. Severe, sudden movement can contaminate your analysis quite severely.</p>
<a class="reference internal image-reference" href="_images/realignment_bad.png"><img alt="_images/realignment_bad.png" class="align-center" src="_images/realignment_bad.png" style="width: 400pt;" /></a>
<p>Motion correction tries to correct for smaller movements, but sometimes it’s best to just remove the images acquired during extreme rapid movement. We use <strong>Artifact Detection</strong> to identify the timepoints/images of the functional image that vary so much they should be excluded from further analysis and to label them so they are excluded from subsequent analyses.</p>
<p>For example, checking the translation and rotation graphs for a session shown above for sudden movement greater than 2 standard deviations from the mean, or for movement greater than 1mm, artifact detection would show that images 16-19, 21, 22 and 169-172 should be excluded from further analysis. The graph produced by artifact detection, with vertical lines corresponding to images with drastic variation is shown below.</p>
<img alt="_images/artifact_detection.png" class="align-center" src="_images/artifact_detection.png" />
</div>
<div class="section" id="coregistration">
<h4>Coregistration<a class="headerlink" href="#coregistration" title="Permalink to this headline">¶</a></h4>
<p>Motion correction aligns all the images within a volume so they are ‘aligned’. Coregistration aligns the functional image with the reference structural image. If you think of the functional image as having been printed on tracing paper, coregistration moves that image around on the reference image until the alignment is at its best. In other words, coregistration tries to superimpose the functional image perfectly on the anatomical image. This allows further transformations of the anatomical image, such as normalization, to be directly applied to the functional image.</p>
<p>The following picture shows an example of good (top) and bad (bottom) coregistration of functional images with the corresponding anatomical images. The red lines are the outline of the cortical folds of the anatomical image superimposed on the underlying greyscale functional image.</p>
<a class="reference internal image-reference" href="_images/coregistration.png"><img alt="_images/coregistration.png" class="align-center" src="_images/coregistration.png" style="width: 400pt;" /></a>
</div>
<div class="section" id="normalization">
<h4>Normalization<a class="headerlink" href="#normalization" title="Permalink to this headline">¶</a></h4>
<p>Every person’s brain is slightly different from every other’s. Brains differ in size and shape. To compare the images of one person’s brain to another’s, the images must first be translated onto a common shape and size, which is called <strong>normalization</strong>. Normalization maps data from the individual subject-space it was measured in onto a reference-space. Once this step is completed, a group analysis or comparison among data can be performed. There are different ways to normalize data but it always includes a template and a source image.</p>
<a class="reference internal image-reference" href="_images/normalization.png"><img alt="_images/normalization.png" class="align-center" src="_images/normalization.png" style="width: 600pt;" /></a>
<ul class="simple">
<li><p>The <strong>template</strong> image is the standard brain in reference-space onto which you want to map your data. This can be a Talairach-, MNI-, or SPM-template, or some other reference image you choose to use.</p></li>
<li><p>The <strong>source</strong> image (normally a higher resolution structural image) is used to calculate the transformation matrix necessary to map the source image onto the template image. This transformation matrix is then used to map the rest of your images (functional and structural) into the reference-space.</p></li>
</ul>
</div>
<div class="section" id="smoothing">
<h4>Smoothing<a class="headerlink" href="#smoothing" title="Permalink to this headline">¶</a></h4>
<p>Structural as well as functional images are smoothed by applying a filter to the image. Smoothing increases the signal to noise ratio of your data by filtering the highest frequencies from the frequency domain; that is, removing the smallest scale changes among voxels. That helps to make the larger scale changes more apparent. There is some inherent variability in functional location among individuals, and smoothing helps to reduce spatial differences between subjects and therefore aids comparing multiple subjects. The trade-off, of course, is that you lose resolution by smoothing. Keep in mind, though, that smoothing can cause regions that are functionally different to combine with each other. In such cases a surface based analysis with smoothing on the surface might be a better choice.</p>
<a class="reference internal image-reference" href="_images/smoothed.png"><img alt="_images/smoothed.png" class="align-center" src="_images/smoothed.png" style="width: 500pt;" /></a>
<a class="reference internal image-reference" href="_images/kernel.png"><img alt="_images/kernel.png" class="align-right" src="_images/kernel.png" style="width: 200pt;" /></a>
<p>Smoothing is implemented by applying a 3D Gaussian kernel to the image, and the amount of smoothing is typically determined by its full width at half maximum (<strong>FWHM</strong>) parameter. As the name implies, FWHM is the width/diameter of the smoothing kernel at half of its height. Each voxel’s value is changed to the result of applying this smoothing kernel to its original value.</p>
<p>Choosing the size of the smoothing kernel also depends on your reason for smoothing. If you want to study a small region, a large kernel might smooth your data too much. The filter shouldn’t generally be larger than the activation you’re trying to detect. Thus, the amount of smoothing that you should use is determined partly by the question you want to answer. Some authors suggest using twice the voxel dimensions as a reasonable starting point.</p>
</div>
<div class="section" id="segmentation-smri-only">
<h4>Segmentation (sMRI only)<a class="headerlink" href="#segmentation-smri-only" title="Permalink to this headline">¶</a></h4>
<a class="reference internal image-reference" href="_images/segmentation.gif"><img alt="_images/segmentation.gif" class="align-right" src="_images/segmentation.gif" style="width: 200pt;" /></a>
<p>Segmentation is the process by which a brain is divided into neurological sections according to a given template specification. This can be rather general, for example, segmenting the brain into gray matter, white matter and cerebrospinal fluid, as is done with SPM’s Segmentation, or quite detailed, segmenting into specific functional regions and their subregions, as is done with FreeSurfer’s <code class="docutils literal notranslate"><span class="pre">recon-all</span></code>, and that is illustrated in the figure.</p>
<p>Segmentation can be used for different things. You can use the segmentation to aid the normalization process or use it to aid further analysis by using a specific segmentation as a mask or as the definition of a specific region of interest (ROI).</p>
</div>
</div>
<div class="section" id="step-2-model-specification-and-estimation">
<h3>Step 2: Model Specification and Estimation<a class="headerlink" href="#step-2-model-specification-and-estimation" title="Permalink to this headline">¶</a></h3>
<p>To test our hypothesis on our data we first need to specify a model that incorporates this hypothesis and accounts for multiple factors such as the expected function of the BOLD signal, the movement during measurement, experiment specify parameters and other regressors and covariates. Such a model is usually represented by a Generalized Linear Model (GLM).</p>
<div class="section" id="the-general-linear-model">
<h4>The General Linear Model<a class="headerlink" href="#the-general-linear-model" title="Permalink to this headline">¶</a></h4>
<p>A GLM describes a response (y), such as the BOLD response in a voxel, in terms of all its contributing factors (xβ) in a linear combination, whilst also accounting for the contribution of error (ε). The column (y) corresponds to one voxel and one row in this column corresponds to one time-point.</p>
<a class="reference internal image-reference" href="_images/GLM.png"><img alt="_images/GLM.png" class="align-center" src="_images/GLM.png" style="width: 300pt;" /></a>
<ul class="simple">
<li><dl class="simple">
<dt><strong>y = dependent variable</strong></dt><dd><p>observed data (e.g. BOLD response in a single voxel)</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>X = Independent Variable</strong> (aka. Predictor)</dt><dd><p>e.g. <em>experimental conditions</em> (embodies all available knowledge about experimentally controlled factors and potential confounds), <em>stimulus information</em> (onset and duration of stimuli), <em>expected shape of BOLD response</em></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>β = Parameters</strong> (aka regression coefficient/beta weights)</dt><dd><p>Quantifies how much each predictor (<em>X</em>) independently influences the dependent variable (<em>Y</em>)</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>ε = Error</strong></dt><dd><p>Variance in the data (<em>Y</em>) which is not explained by the linear combination of predictors (<em>Xβ</em>). The error is assumed to be normally distributed.</p>
</dd>
</dl>
</li>
</ul>
<p>The predictor variables are stored in a so called <strong>Design Matrix</strong>. The <strong>β</strong> parameters define the contribution of each component of this design matrix to the model. They are estimated so as to minimize the error, and are used to generate the <strong>contrasts</strong> between conditions. The <strong>Errors</strong> is the difference between the observed data and the model defined by Xβ.</p>
</div>
<div class="section" id="potential-problems-of-the-glm-approach">
<h4>Potential problems of the GLM approach<a class="headerlink" href="#potential-problems-of-the-glm-approach" title="Permalink to this headline">¶</a></h4>
<p><strong>BOLD responses have a delayed and dispersed form</strong></p>
<ul class="simple">
<li><p>We have to take the time delay and the HRF shape of the BOLD response into account when we create our design matrix.</p></li>
</ul>
<p><strong>BOLD signals include substantial amounts of low-frequency noise</strong></p>
<ul class="simple">
<li><p>By high pass filtering our data and adding time regressors of 1st, 2nd,… order we can correct for low-frequency drifts in our measured data. This low frequency signals are caused by non-experimental effects, such as scanner drift etc.</p></li>
</ul>
<a class="reference internal image-reference" href="_images/time.png"><img alt="_images/time.png" class="align-center" src="_images/time.png" style="width: 350pt;" /></a>
<p>This <strong>High pass Filter</strong> is established by setting up discrete cosine functions over the time period of your acquisition. In the example below you see a constant term of 1, followed by half of a cosine function increasing by half a period for each following curve. Such regressors correct for the influence of changes in the low-frequency spectrum.</p>
<a class="reference internal image-reference" href="_images/highpassfilter.png"><img alt="_images/highpassfilter.png" class="align-center" src="_images/highpassfilter.png" style="width: 250pt;" /></a>
</div>
<div class="section" id="example-of-a-design-matrix">
<h4>Example of a Design Matrix<a class="headerlink" href="#example-of-a-design-matrix" title="Permalink to this headline">¶</a></h4>
<a class="reference internal image-reference" href="_images/stimuli.png"><img alt="_images/stimuli.png" class="align-right" src="_images/stimuli.png" style="width: 200pt;" /></a>
<p>Let us assume we have an experiment where we present subjects faces of humans and animals alike. Our goal is to measure the difference between the brain activation when a face of an animal is presented in contrast to the activation of the brain when a human face is presented. Our experiment is set up in such a way that subjects have two different blocks of stimuli presentation. In both blocks there are timepoints where faces of humans, faces of animals and no faces (resting state) are presented.</p>
<p>Now, we combine all that we know about our model into one single Design Matrix. This Matrix contains multiple columns, which contain information about the stimuli (onset, duration and curve function of the BOLD-signal i.e. the shape of the HRF). In our example column <em>Sn(1) humans</em> and <em>Sn(1) animals</em> code for the stimuli of humans and animals during the first session of our fictive experiment. Accordingly, Sn(2) codes for all the regressors in the second session. <em>Sn(1) resting</em> codes for the timepoints where subjects weren’t presented any stimuli.</p>
<a class="reference internal image-reference" href="_images/designmatrix.png"><img alt="_images/designmatrix.png" class="align-center" src="_images/designmatrix.png" style="width: 350pt;" /></a>
<p>The y-axis codes for the measured scan or the passed time, depending on the specification of your design. The x-axis stands for all the regressors that we specified.</p>
<p>The regressors <em>Sn(1) R1</em> to <em>Sn(1) R6</em> stand for the movement parameters we got from the realignment process. The regressors <em>Sn(1) linear</em>, <em>Sn(1) quadratic</em>, <em>Sn(1) cubic</em> and <em>Sn(1) quartic</em> are just examples of correction for the low frequency in your data. If you are using a high-pass filter of e.g. 128 seconds you don’t need to specifically include those regressors in your design matrix.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Adding one more regressors to your model decrease the degrees of freedom in your statistical tests by one.</p>
</div>
</div>
<div class="section" id="model-estimation">
<h4>Model Estimation<a class="headerlink" href="#model-estimation" title="Permalink to this headline">¶</a></h4>
<p>After we specified the parameters of our model in a design matrix we are ready to estimate our model. This means that we apply our model on the time course of each and every voxel.</p>
<p>Depending on the software you are using you might get different types of results. If you are using <strong>SPM</strong> the following images are created each time an analysis is performed (1st or 2nd level):</p>
<ul class="simple">
<li><dl class="simple">
<dt><strong>beta images</strong></dt><dd><p>images of estimated regression coefficients (parameter estimate). beta images contain information about the size of the effect of interest. A given voxel in each beta image will have a value related to the size of effect for that explanatory variable.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>error image</strong> - <code class="docutils literal notranslate"><span class="pre">ResMS</span></code>-image</dt><dd><p>residual sum of squares or variance image. It is a measure of within-subject error at the 1st level or between-subject error at the 2nd level analysis. This image is used to produce spmT images.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>con images</strong> - <code class="docutils literal notranslate"><span class="pre">con</span></code>-images</dt><dd><p>during contrast estimation beta images are linearly combined to produce relevant <code class="docutils literal notranslate"><span class="pre">con</span></code>-images</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>T images</strong> - <code class="docutils literal notranslate"><span class="pre">spmT</span></code>-images</dt><dd><p>during contrast estimation the beta values of a <code class="docutils literal notranslate"><span class="pre">con</span></code>-image are combined with error values of the <code class="docutils literal notranslate"><span class="pre">ResMS</span></code>-image to calculate the t-value at each voxel</p>
</dd>
</dl>
</li>
</ul>
</div>
</div>
<div class="section" id="step-3-statistical-inference">
<h3>Step 3: Statistical Inference<a class="headerlink" href="#step-3-statistical-inference" title="Permalink to this headline">¶</a></h3>
<p>Before we go into the specifics of a statistical analysis, let me explain you the difference between a 1st and a 2nd level analysis.</p>
<dl class="simple">
<dt><strong>1st level analysis (within-subject)</strong></dt><dd><p>A 1st level analysis is the statistical analysis done on each and every subject by itself. For this procedure the data doesn’t have to be normalized, i.e in a common reference space. A design matrix on this level controls for subject specific parameters as movement, respiration, heart beat, etc.</p>
</dd>
<dt><strong>2nd level analysis (between-subject)</strong></dt><dd><p>A 2nd level analysis is the statistical analysis done on the group. To be able to do this, our subject specific data has to be normalized and transformed from subject-space into reference-space. Otherwise we wouldn’t be able to compare subjects between each other. Additionally, all contrasts of the 1st level analysis have to be estimated because the model of the 2nd level analysis is conducted on them. The design matrix of the 2nd level analysis controls for subject specific parameters such as age, gender, socio-economic parameters, etc. At this point we also specify the group assignment of each subject.</p>
</dd>
</dl>
<div class="section" id="contrast-estimation">
<h4>Contrast Estimation<a class="headerlink" href="#contrast-estimation" title="Permalink to this headline">¶</a></h4>
<a class="reference internal image-reference" href="_images/contrasts.png"><img alt="_images/contrasts.png" class="align-right" src="_images/contrasts.png" style="width: 220pt;" /></a>
<p>Independent of the level of your analysis, after you’ve specified and estimated your model you now have to estimate the contrasts you are interested in. In such a <strong>contrast</strong> you specify how to weight the different regressors of your design matrix and combine them in one single image.</p>
<p>For example, if you want to compare the brain activation during the presentation of human faces compared to the brain activation during the presentation of animal faces over two sessions you have to weight the regressors <em>Sn(1) humans</em> and <em>Sn(2) humans</em> with 1 and <em>Sn(1) animals</em> and <em>Sn(2) animals</em> with -1, as can be seen in <strong>contrast 3</strong>. This will subtract the value of the animal-activation from the activation during the presentation of human faces. The result is an image where the positive activation stands for “more active” during the presentation of human faces than during the presentation of animal faces.</p>
<p>Contrast 1 codes for <em>human faces vs. resting</em>, contrast 2 codes for <em>animal faces vs. resting</em>, contrast 4 codes for <em>animal faces vs. human faces</em> (which is just the inverse image of contrast 3) and contrast 5 codes for <em>session 1 vs. session 2</em>, which looks for regions which were more active in the first session than in the second session.</p>
</div>
<div class="section" id="thresholding">
<h4>Thresholding<a class="headerlink" href="#thresholding" title="Permalink to this headline">¶</a></h4>
<p>After the contrasts are estimated there is only one final step to be taken before you get a scientific based answer to your question. You have to threshold your results. With that I mean, you have to specify the level of significance you want to test your data on, you have to correct for multiple comparison and you have to specify the parameters of the results you are looking for. E.g.:</p>
<ul class="simple">
<li><p><strong>FWE-correction</strong>: The family-wise error correction is one way to correct for multiple comparisons</p></li>
<li><p><strong>p-value</strong>: specify the hight of the significance threshold that you want to use (e.g. z=1.6449 equals p&lt;0.05 (one-tailed); see image)</p></li>
<li><p><strong>voxel extend</strong>: specify the minimum size of a “significant” cluster by specifying the number of voxel it at least has to contain.</p></li>
</ul>
<a class="reference internal image-reference" href="_images/pvalues.png"><img alt="_images/pvalues.png" class="align-center" src="_images/pvalues.png" style="width: 350pt;" /></a>
<p>If you do all this correctly, you’ll end up with something as shown in the following picture. The picture shows you the average brain activation of 20 subjects during the presentation of an acoustic stimuli. The p-value are shown from red to yellow, representing values from 0.05 to 0.00. Shown are only cluster with a voxel extend of at least 100 voxels.</p>
<a class="reference internal image-reference" href="_images/contrast_acoustic.png"><img alt="_images/contrast_acoustic.png" class="align-center" src="_images/contrast_acoustic.png" style="width: 350pt;" /></a>
</div>
</div>
</div>
</div>



          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="nipypeAndNeuroimaging.html" title="Nipype and Neuroimaging"
             >next</a></li>
        <li class="right" >
          <a href="nipype.html" title="Introduction to Nipype"
             >previous</a> |</li>
        <li><a href="index.html">Home</a>|</li>
        <li><a href="http://miykael.github.com/nipype-beginner-s-guide/tableofcontent.html">Table of Contents</a>|</li>
        <li><a href="http://miykael.github.com/nipype-beginner-s-guide/faq.html">FAQ</a>|</li>
        <li><a href="http://miykael.github.com/nipype-beginner-s-guide/glossary.html">Glossary</a>|</li>
        <li><a href="https://github.com/miykael/nipype-beginner-s-guide/">github</a>|</li>
        <li><a href="http://nipy.org/nipype/">Nipype</a>|</li>
        <li><a href="http://miykael.github.io/nipype-beginner-s-guide/help.html">Help</a>|</li> 
      </ul>
    </div>
    <div class="footer">
    <p>
        &copy; Copyright 2016, Michael Notter.
      Last updated on June 10, 2021.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 3.2.1.
    </p>
    <p>
    This page uses <a href="http://analytics.google.com/">Google Analytics</a> to collect statistics. You can disable it by blocking the JavaScript coming from www.google-analytics.com.
    </p>
    </div>
  </body>
</html>